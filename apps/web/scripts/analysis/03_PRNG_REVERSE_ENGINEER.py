#!/usr/bin/env python3
"""
Phase 3: PRNG Reverse Engineering
===================================
Determine HOW the Anna Matrix was generated.

Key insight: With 99.58% point symmetry (matrix[r,c] + matrix[127-r,127-c] = -1),
only ~8,226 values are independent. The rest are deterministic.

This script tests whether the independent values could have been generated by
known PRNGs, and runs standard randomness tests on them.
"""
import sys
import os
import json
import numpy as np
from collections import Counter
from pathlib import Path
from scipy import stats

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from lib.matrix_loader import load_matrix
from lib.statistical_tests import TestReport

SEED = 42
OUTPUT_DIR = Path(__file__).parent
RESULTS_FILE = OUTPUT_DIR / "03_PRNG_REVERSE_ENGINEER_RESULTS.json"


def extract_independent_half(matrix):
    """Extract independent values from the matrix.

    For point-symmetric matrix where matrix[r,c] + matrix[127-r,127-c] = -1:
    - For each pair (r,c) and (127-r, 127-c), keep the one with smaller (r,c) lexicographic order
    - Also keep exception cells where the symmetry breaks

    Returns: (independent_values, exception_pairs)
    """
    n = matrix.shape[0]
    independent = []
    exceptions = []

    visited = set()
    for r in range(n):
        for c in range(n):
            if (r, c) in visited:
                continue
            r2, c2 = n - 1 - r, n - 1 - c
            visited.add((r, c))
            visited.add((r2, c2))

            val1 = int(matrix[r, c])
            val2 = int(matrix[r2, c2])
            expected_sum = -1

            if (r, c) == (r2, c2):
                # Self-mirror (only possible if n is odd, not for 128)
                independent.append(val1)
            elif (r, c) < (r2, c2):
                independent.append(val1)
                if val1 + val2 != expected_sum:
                    exceptions.append({
                        "pos1": (r, c), "val1": val1,
                        "pos2": (r2, c2), "val2": val2,
                        "sum": val1 + val2,
                        "deviation": val1 + val2 - expected_sum
                    })
            else:
                independent.append(val2)
                if val1 + val2 != expected_sum:
                    exceptions.append({
                        "pos1": (r2, c2), "val1": val2,
                        "pos2": (r, c), "val2": val1,
                        "sum": val1 + val2,
                        "deviation": val1 + val2 - expected_sum
                    })

    return np.array(independent), exceptions


def frequency_test(data):
    """NIST frequency (monobit) test - are 0s and 1s balanced in bits?"""
    bits = []
    for val in data:
        byte_val = val & 0xFF  # treat as unsigned
        for b in range(8):
            bits.append((byte_val >> b) & 1)
    bits = np.array(bits)
    n = len(bits)
    s = np.sum(2 * bits - 1)  # convert 0->-1, 1->+1
    s_obs = abs(s) / np.sqrt(n)
    p_value = 2 * (1 - stats.norm.cdf(s_obs))
    return {"test": "frequency", "s_obs": float(s_obs), "p_value": float(p_value)}


def runs_test(data):
    """NIST runs test - are there too many or too few runs of 0s and 1s?"""
    bits = []
    for val in data:
        byte_val = val & 0xFF
        for b in range(8):
            bits.append((byte_val >> b) & 1)
    bits = np.array(bits)
    n = len(bits)
    pi = np.mean(bits)

    # Pre-test: if pi is too far from 0.5, skip
    if abs(pi - 0.5) >= 2 / np.sqrt(n):
        return {"test": "runs", "p_value": 0.0, "note": "frequency pre-test failed"}

    # Count runs
    runs = 1
    for i in range(1, n):
        if bits[i] != bits[i - 1]:
            runs += 1

    expected_runs = 2 * n * pi * (1 - pi) + 1
    std_runs = 2 * np.sqrt(2 * n) * pi * (1 - pi)
    if std_runs == 0:
        return {"test": "runs", "p_value": 0.0, "note": "zero std"}

    z = (runs - expected_runs) / std_runs
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))
    return {"test": "runs", "z": float(z), "p_value": float(p_value)}


def serial_test(data, m=2):
    """Serial test - check distribution of m-bit patterns."""
    bits = []
    for val in data:
        byte_val = val & 0xFF
        for b in range(8):
            bits.append((byte_val >> b) & 1)
    bits = np.array(bits)
    n = len(bits)

    def psi_sq(bits, m):
        if m == 0:
            return 0
        n = len(bits)
        counts = Counter()
        for i in range(n):
            pattern = tuple(bits[i:i + m] if i + m <= n else
                           np.concatenate([bits[i:], bits[:m - (n - i)]]))
            if len(pattern) == m:
                counts[pattern] += 1
        return (2**m / n) * sum(c**2 for c in counts.values()) - n

    psi_m = psi_sq(bits, m)
    psi_m1 = psi_sq(bits, m - 1)
    psi_m2 = psi_sq(bits, m - 2) if m >= 2 else 0

    delta1 = psi_m - psi_m1
    delta2 = psi_m - 2 * psi_m1 + psi_m2

    p1 = 1 - stats.chi2.cdf(abs(delta1), 2**(m - 1))
    p2 = 1 - stats.chi2.cdf(abs(delta2), 2**(m - 2))

    return {"test": f"serial_m{m}", "delta1": float(delta1), "delta2": float(delta2),
            "p_value_1": float(p1), "p_value_2": float(p2)}


def autocorrelation_test(data, lag=1):
    """Test for autocorrelation in the sequence."""
    x = np.array(data, dtype=np.float64)
    n = len(x)
    x_mean = np.mean(x)
    x_centered = x - x_mean
    var = np.sum(x_centered ** 2)
    if var == 0:
        return {"test": f"autocorrelation_lag{lag}", "r": 0.0, "p_value": 1.0}

    r = np.sum(x_centered[:n - lag] * x_centered[lag:]) / var
    # Under null hypothesis (no correlation), r ~ N(0, 1/n)
    z = r * np.sqrt(n)
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))
    return {"test": f"autocorrelation_lag{lag}", "r": float(r), "z": float(z),
            "p_value": float(p_value)}


def byte_distribution_test(data):
    """Chi-squared test: are byte values uniformly distributed?"""
    counts = Counter(int(v) & 0xFF for v in data)
    observed = np.array([counts.get(i, 0) for i in range(256)])
    expected = np.full(256, len(data) / 256)
    chi2, p_value = stats.chisquare(observed, expected)
    return {"test": "byte_distribution", "chi2": float(chi2), "p_value": float(p_value)}


def signed_distribution_test(data):
    """Chi-squared test: are signed values uniformly distributed in [-128, 127]?"""
    counts = Counter(int(v) for v in data)
    observed = np.array([counts.get(i, 0) for i in range(-128, 128)])
    expected = np.full(256, len(data) / 256)
    chi2, p_value = stats.chisquare(observed, expected)
    return {"test": "signed_distribution", "chi2": float(chi2), "p_value": float(p_value)}


def longest_run_test(data):
    """Test for longest run of ones in bits."""
    bits = []
    for val in data:
        byte_val = val & 0xFF
        for b in range(8):
            bits.append((byte_val >> b) & 1)

    # Count longest run of 1s
    max_run = 0
    current_run = 0
    for b in bits:
        if b == 1:
            current_run += 1
            max_run = max(max_run, current_run)
        else:
            current_run = 0

    # Expected longest run in n bits is approximately log2(n)
    n = len(bits)
    expected = np.log2(n) if n > 0 else 0

    return {"test": "longest_run_of_ones", "longest_run": int(max_run),
            "expected_approx": float(expected), "n_bits": n}


def test_lcg_match(data, n_seeds=10000):
    """Test if data matches Linear Congruential Generator output."""
    # Common LCG parameters
    lcg_params = [
        (1103515245, 12345, 2**31),  # glibc
        (214013, 2531011, 2**32),     # MSVC
        (16807, 0, 2**31 - 1),        # MINSTD
        (1664525, 1013904223, 2**32),  # Numerical Recipes
    ]

    best_match = {"name": "none", "score": 0, "seed": 0}

    for name_idx, (a, c, m) in enumerate(lcg_params):
        names = ["glibc", "MSVC", "MINSTD", "NumRecipes"]
        for seed in range(min(n_seeds, 10000)):
            state = seed
            match_count = 0
            for i in range(min(100, len(data))):
                state = (a * state + c) % m
                # Convert to signed byte
                lcg_byte = (state >> 16) % 256
                if lcg_byte > 127:
                    lcg_byte -= 256
                if lcg_byte == data[i]:
                    match_count += 1

            if match_count > best_match["score"]:
                best_match = {
                    "name": names[name_idx],
                    "score": match_count,
                    "seed": seed,
                    "params": (a, c, m),
                    "match_rate": match_count / min(100, len(data))
                }

    return best_match


def main():
    print("=" * 80)
    print("  PHASE 3: PRNG REVERSE ENGINEERING")
    print("=" * 80)
    print()

    matrix = load_matrix(verify_hash=False)

    # =========================================================================
    # SECTION 1: EXTRACT INDEPENDENT HALF
    # =========================================================================
    print("SECTION 1: EXTRACTING INDEPENDENT VALUES")
    print("-" * 80)

    independent, exceptions = extract_independent_half(matrix)

    print(f"Total cells: {128 * 128} = {128 * 128}")
    print(f"Independent values: {len(independent)}")
    print(f"Exception pairs: {len(exceptions)}")
    print(f"Compression ratio: {len(independent) / (128*128) * 100:.1f}%")
    print(f"  -> Only {len(independent)} values needed to reconstruct entire {128*128}-cell matrix")
    print()

    print(f"Independent value stats:")
    print(f"  Range: [{independent.min()}, {independent.max()}]")
    print(f"  Mean: {independent.mean():.2f}")
    print(f"  Std: {independent.std():.2f}")
    print(f"  Unique: {len(np.unique(independent))}")

    # =========================================================================
    # SECTION 2: RANDOMNESS TESTS ON INDEPENDENT HALF
    # =========================================================================
    print("\n" + "=" * 80)
    print("SECTION 2: NIST-STYLE RANDOMNESS TESTS")
    print("-" * 80)
    print()
    print("If the independent half passes all randomness tests, it is consistent")
    print("with being generated by a good PRNG. If it fails specific tests, the")
    print("failure pattern may identify the generator or reveal structure.")
    print()

    test_results = {}

    # Frequency test
    freq = frequency_test(independent)
    test_results["frequency"] = freq
    print(f"1. Frequency (monobit): S_obs={freq['s_obs']:.4f}, p={freq['p_value']:.4f} "
          f"{'PASS' if freq['p_value'] > 0.01 else 'FAIL'}")

    # Runs test
    runs = runs_test(independent)
    test_results["runs"] = runs
    print(f"2. Runs test: p={runs['p_value']:.4f} "
          f"{'PASS' if runs['p_value'] > 0.01 else 'FAIL'}")

    # Serial test
    ser = serial_test(independent, m=2)
    test_results["serial"] = ser
    print(f"3. Serial test (m=2): p1={ser['p_value_1']:.4f}, p2={ser['p_value_2']:.4f} "
          f"{'PASS' if min(ser['p_value_1'], ser['p_value_2']) > 0.01 else 'FAIL'}")

    # Autocorrelation tests
    for lag in [1, 2, 4, 8, 16]:
        ac = autocorrelation_test(independent, lag)
        test_results[f"autocorrelation_lag{lag}"] = ac
        print(f"4. Autocorrelation (lag={lag:2d}): r={ac['r']:.6f}, p={ac['p_value']:.4f} "
              f"{'PASS' if ac['p_value'] > 0.01 else 'FAIL'}")

    # Byte distribution
    byte_dist = byte_distribution_test(independent)
    test_results["byte_distribution"] = byte_dist
    print(f"5. Byte distribution (chi2): chi2={byte_dist['chi2']:.1f}, p={byte_dist['p_value']:.4f} "
          f"{'PASS' if byte_dist['p_value'] > 0.01 else 'FAIL'}")

    # Signed distribution
    signed_dist = signed_distribution_test(independent)
    test_results["signed_distribution"] = signed_dist
    print(f"6. Signed distribution: chi2={signed_dist['chi2']:.1f}, p={signed_dist['p_value']:.4f} "
          f"{'PASS' if signed_dist['p_value'] > 0.01 else 'FAIL'}")

    # Longest run
    lr = longest_run_test(independent)
    test_results["longest_run"] = lr
    print(f"7. Longest run of 1s: {lr['longest_run']} (expected ~{lr['expected_approx']:.0f} for {lr['n_bits']} bits)")

    # Summary
    n_pass = sum(1 for t in test_results.values()
                 if isinstance(t.get("p_value"), float) and t["p_value"] > 0.01)
    n_total = sum(1 for t in test_results.values()
                  if isinstance(t.get("p_value"), float))
    print(f"\nRANDOMNESS SUMMARY: {n_pass}/{n_total} tests PASS (p > 0.01)")

    if n_pass == n_total:
        print("-> Independent half is CONSISTENT with PRNG output")
    else:
        print("-> Independent half shows NON-RANDOM structure")
        failing = [t for t in test_results.values()
                   if isinstance(t.get("p_value"), float) and t["p_value"] <= 0.01]
        print(f"   Failing tests: {[t['test'] for t in failing]}")

    # =========================================================================
    # SECTION 3: LCG SEED SEARCH
    # =========================================================================
    print("\n" + "=" * 80)
    print("SECTION 3: LCG SEED SEARCH")
    print("-" * 80)
    print()
    print("Testing first 10,000 seeds for 4 common LCG algorithms...")

    lcg_result = test_lcg_match(independent.tolist(), n_seeds=10000)
    print(f"\nBest LCG match:")
    print(f"  Algorithm: {lcg_result['name']}")
    print(f"  Seed: {lcg_result['seed']}")
    print(f"  Match rate: {lcg_result['match_rate']*100:.1f}% (first 100 values)")
    print(f"  Expected random match: {1/256*100:.2f}%")

    if lcg_result["match_rate"] > 0.05:
        print("  -> POSSIBLE LCG MATCH (above 5% threshold)")
    else:
        print("  -> No LCG match found (match rate consistent with random)")

    # =========================================================================
    # SECTION 4: CONSTRUCTION HYPOTHESIS
    # =========================================================================
    print("\n" + "=" * 80)
    print("SECTION 4: CONSTRUCTION HYPOTHESIS")
    print("-" * 80)
    print()

    print("Three hypotheses for how the matrix was built:")
    print()
    print("Hypothesis A: Generate ~8,192 random values, apply symmetry rule")
    print("  -> Would explain: point symmetry, random-looking independent half")
    print("  -> Would NOT explain: Row 6 bias, low entropy rows, 68 exceptions")
    print()
    print("Hypothesis B: Generate full 16,384 random values, impose symmetry on most,")
    print("  then manually edit specific cells (Row 6, exceptions)")
    print("  -> Would explain: everything including Row 6 bias and exceptions")
    print("  -> Evidence: exceptions and Row 6 bias are the 'manual edits'")
    print()
    print("Hypothesis C: Entirely hand-crafted")
    print("  -> Would explain: everything")
    print("  -> But: 8,192 truly random-looking values is hard to hand-craft")
    print()

    # Evidence for each hypothesis
    print("EVIDENCE ASSESSMENT:")
    print(f"  Randomness tests pass rate: {n_pass}/{n_total}")
    if n_pass >= n_total - 1:
        print("  -> Favors Hypothesis A or B (looks like PRNG output)")
    else:
        print("  -> Disfavors pure PRNG generation (some structure detected)")

    print(f"  Exception count: {len(exceptions)} (0.42% of matrix)")
    print(f"  Row 6 bias: 24/128 cells = value 26 (18.8%)")
    print("  -> Row 6 bias and exceptions are likely deliberate modifications")
    print("  -> Favors Hypothesis B: PRNG base + manual edits")

    # =========================================================================
    # SECTION 5: EXCEPTION ANALYSIS
    # =========================================================================
    print("\n" + "=" * 80)
    print("SECTION 5: EXCEPTION PAIR DETAILS")
    print("-" * 80)
    print()

    print(f"{'Pos 1':>10} {'Val1':>5} {'Pos 2':>10} {'Val2':>5} {'Sum':>5} {'Dev':>5}")
    print("-" * 45)
    deviations = []
    for exc in exceptions:
        p1 = f"({exc['pos1'][0]:3d},{exc['pos1'][1]:3d})"
        p2 = f"({exc['pos2'][0]:3d},{exc['pos2'][1]:3d})"
        print(f"{p1:>10} {exc['val1']:5d} {p2:>10} {exc['val2']:5d} {exc['sum']:5d} {exc['deviation']:+5d}")
        deviations.append(exc['deviation'])

    deviations = np.array(deviations)
    print(f"\nDeviation statistics:")
    print(f"  Count: {len(deviations)}")
    print(f"  Mean: {deviations.mean():.2f}")
    print(f"  Std: {deviations.std():.2f}")
    print(f"  Min: {deviations.min()}, Max: {deviations.max()}")
    print(f"  Unique values: {sorted(np.unique(deviations).tolist())}")

    # =========================================================================
    # SAVE RESULTS
    # =========================================================================
    results = {
        "independent_count": int(len(independent)),
        "exception_count": len(exceptions),
        "randomness_tests": {k: {kk: (float(vv) if isinstance(vv, (np.floating, float)) else vv)
                                  for kk, vv in v.items()}
                             for k, v in test_results.items()},
        "randomness_pass_rate": f"{n_pass}/{n_total}",
        "lcg_best_match": {k: (float(v) if isinstance(v, (np.floating, float)) else
                               (list(v) if isinstance(v, tuple) else v))
                           for k, v in lcg_result.items()},
        "exceptions": [
            {k: (list(v) if isinstance(v, tuple) else int(v) if isinstance(v, (np.integer,)) else v)
             for k, v in exc.items()}
            for exc in exceptions
        ],
        "deviation_stats": {
            "mean": float(deviations.mean()),
            "std": float(deviations.std()),
            "unique": sorted(int(x) for x in np.unique(deviations).tolist()),
        },
        "hypothesis": "B (PRNG base + manual edits)" if n_pass >= n_total - 1 else "C (partially structured)",
    }

    with open(str(RESULTS_FILE), "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nResults saved to {RESULTS_FILE}")


if __name__ == "__main__":
    main()
